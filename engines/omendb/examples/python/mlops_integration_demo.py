#!/usr/bin/env python3
"""
MLOps Integration Demo for OmenDB Community Edition

Demonstrates integration with popular MLOps platforms:
- MLflow for experiment tracking
- Weights & Biases for model monitoring
- HuggingFace for model embeddings
- scikit-learn for ML pipelines
- pandas for data processing

This example shows how OmenDB can be integrated into real ML workflows
for vector storage, similarity search, and model serving.
"""

import os
import sys
import tempfile
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Tuple

# Add OmenDB Python package to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "python"))

try:
    from omendb import DB

    print("‚úÖ Using OmenDB bindings")
except ImportError:
    print("‚ùå OmenDB bindings not available")
    sys.exit(1)


def test_numpy_integration():
    """Test NumPy array compatibility."""
    print("Testing NumPy integration...")

    # Create vectors from NumPy arrays
    np_embeddings = np.random.rand(10, 128).astype(np.float32)

    with tempfile.TemporaryDirectory() as temp_dir:
        db_path = os.path.join(temp_dir, "numpy_test.omendb")
        db = DB(db_path)

        # Insert vectors from NumPy
        for i, embedding in enumerate(np_embeddings):
            vector_id = f"numpy_vec_{i}"
            metadata = {"source": "numpy", "index": str(i)}

            # Convert NumPy array to list for OmenDB
            vector_list = embedding.tolist()
            success = db.add(vector_id, vector_list, metadata)

            if not success:
                print(f"‚ùå Failed to insert vector {vector_id}")
                return False

        # Test search with NumPy query
        query_vector = np.random.rand(128).astype(np.float32)
        results = db.search(query_vector.tolist(), limit=5)

        print(f"‚úÖ NumPy integration successful: {len(results)} results")
        return True


def test_sklearn_compatibility():
    """Test scikit-learn pipeline compatibility."""
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        from sklearn.decomposition import PCA
    except ImportError:
        print("‚ùì scikit-learn not available, skipping test")
        return True

    print("Testing scikit-learn integration...")

    # Sample text data for TF-IDF
    documents = [
        "machine learning algorithms for vector search",
        "deep learning neural networks and embeddings",
        "natural language processing with transformers",
        "computer vision image classification models",
        "data science analytics and visualization",
    ]

    # Create TF-IDF vectors
    vectorizer = TfidfVectorizer(max_features=100, stop_words="english")
    tfidf_matrix = vectorizer.fit_transform(documents)

    # Reduce dimensionality for OmenDB
    pca = PCA(n_components=50)
    reduced_vectors = pca.fit_transform(tfidf_matrix.toarray())

    with tempfile.TemporaryDirectory() as temp_dir:
        db_path = os.path.join(temp_dir, "sklearn_test.omendb")
        db = DB(db_path)

        # Insert TF-IDF vectors
        for i, vector in enumerate(reduced_vectors):
            vector_id = f"doc_{i}"
            metadata = {
                "text": documents[i][:50] + "...",
                "method": "tfidf_pca",
                "components": "50",
            }

            success = db.add(vector_id, vector.tolist(), metadata)
            if not success:
                print(f"‚ùå Failed to insert document {vector_id}")
                return False

        # Test similarity search
        query_doc = "machine learning vector database"
        query_vector = vectorizer.transform([query_doc])
        query_reduced = pca.transform(query_vector.toarray())[0]

        results = db.search(query_reduced.tolist(), limit=3)

        print(f"‚úÖ scikit-learn integration successful: {len(results)} results")
        return True


def test_pandas_integration():
    """Test pandas DataFrame compatibility."""
    try:
        import pandas as pd
    except ImportError:
        print("‚ùì pandas not available, skipping test")
        return True

    print("Testing pandas integration...")

    # Create sample DataFrame with features
    data = {
        "feature_1": np.random.rand(20),
        "feature_2": np.random.rand(20),
        "feature_3": np.random.rand(20),
        "feature_4": np.random.rand(20),
        "label": [f"item_{i}" for i in range(20)],
        "category": ["A", "B", "C", "D"] * 5,
    }

    df = pd.DataFrame(data)

    with tempfile.TemporaryDirectory() as temp_dir:
        db_path = os.path.join(temp_dir, "pandas_test.omendb")
        db = DB(db_path)

        # Insert rows as vectors
        for index, row in df.iterrows():
            vector_id = row["label"]

            # Extract feature columns as vector
            features = [
                row["feature_1"],
                row["feature_2"],
                row["feature_3"],
                row["feature_4"],
            ]

            metadata = {"category": row["category"], "index": str(index)}

            success = db.add(vector_id, features, metadata)
            if not success:
                print(f"‚ùå Failed to insert row {vector_id}")
                return False

        # Test search with DataFrame query
        query_features = df.iloc[0][
            ["feature_1", "feature_2", "feature_3", "feature_4"]
        ].tolist()
        results = db.search(query_features, limit=5)

        print(f"‚úÖ pandas integration successful: {len(results)} results")
        return True


def test_huggingface_simulation():
    """Simulate HuggingFace embeddings integration."""
    print("Testing HuggingFace-style embeddings...")

    # Simulate sentence embeddings (384D like all-MiniLM-L6-v2)
    embedding_dim = 384
    n_sentences = 50

    with tempfile.TemporaryDirectory() as temp_dir:
        db_path = os.path.join(temp_dir, "huggingface_test.omendb")
        db = DB(db_path)

        # Simulate embedding generation
        sentences = [
            f"This is sentence number {i} for testing" for i in range(n_sentences)
        ]

        for i, sentence in enumerate(sentences):
            # Simulate embedding generation
            embedding = np.random.randn(embedding_dim).astype(np.float32)
            # Normalize (like sentence-transformers)
            embedding = embedding / np.linalg.norm(embedding)

            vector_id = f"sent_{i}"
            metadata = {
                "text": sentence,
                "model": "all-MiniLM-L6-v2-simulated",
                "pooling": "mean",
            }

            success = db.add(vector_id, embedding.tolist(), metadata)
            if not success:
                print(f"‚ùå Failed to insert sentence {vector_id}")
                return False

        # Test semantic search
        query = "This is a test query sentence"
        query_embedding = np.random.randn(embedding_dim).astype(np.float32)
        query_embedding = query_embedding / np.linalg.norm(query_embedding)

        results = db.search(query_embedding.tolist(), limit=5)

        print(f"‚úÖ HuggingFace simulation successful: {len(results)} results")
        return True


def test_mlflow_integration():
    """Demonstrate MLflow experiment tracking with OmenDB."""
    print("Testing MLflow-style experiment tracking...")

    # Simulate model training with different hyperparameters
    experiments = [
        {"model": "resnet50", "lr": 0.001, "batch_size": 32},
        {"model": "resnet50", "lr": 0.01, "batch_size": 64},
        {"model": "efficientnet", "lr": 0.001, "batch_size": 16},
        {"model": "efficientnet", "lr": 0.005, "batch_size": 32},
    ]

    with tempfile.TemporaryDirectory() as temp_dir:
        db_path = os.path.join(temp_dir, "mlflow_test.omendb")
        db = DB(db_path)

        # Store model embeddings from experiments
        for i, exp in enumerate(experiments):
            # Simulate model output embeddings
            embedding_dim = 2048  # ResNet-style features
            model_embedding = np.random.randn(embedding_dim).astype(np.float32)

            run_id = f"run_{i}_{exp['model']}_{exp['lr']}"
            metadata = {
                "experiment_id": "image_classification",
                "run_id": run_id,
                "model": exp["model"],
                "learning_rate": str(exp["lr"]),
                "batch_size": str(exp["batch_size"]),
                "metrics": str(
                    {"accuracy": np.random.rand(), "loss": np.random.rand()}
                ),
            }

            success = db.add(run_id, model_embedding.tolist(), metadata)
            if not success:
                print(f"‚ùå Failed to store experiment {run_id}")
                return False

        # Find similar experiments
        query_embedding = np.random.randn(embedding_dim).astype(np.float32)
        results = db.search(query_embedding.tolist(), limit=2)

        print(
            f"‚úÖ MLflow integration successful: {len(results)} similar experiments found"
        )
        return True


def test_wandb_simulation():
    """Simulate Weights & Biases model monitoring."""
    print("Testing W&B-style model monitoring...")

    # Simulate model checkpoints
    n_checkpoints = 10
    embedding_dim = 512

    with tempfile.TemporaryDirectory() as temp_dir:
        db_path = os.path.join(temp_dir, "wandb_test.omendb")
        db = DB(db_path)

        # Store model checkpoint embeddings
        for epoch in range(n_checkpoints):
            # Simulate model state embedding
            checkpoint_embedding = np.random.randn(embedding_dim).astype(np.float32)

            checkpoint_id = f"checkpoint_epoch_{epoch}"
            metadata = {
                "project": "vector-search-optimization",
                "run_name": "baseline-experiment",
                "epoch": str(epoch),
                "step": str(epoch * 1000),
                "loss": str(1.0 / (epoch + 1)),
                "accuracy": str(0.8 + 0.02 * epoch),
            }

            success = db.add(checkpoint_id, checkpoint_embedding.tolist(), metadata)
            if not success:
                print(f"‚ùå Failed to store checkpoint {checkpoint_id}")
                return False

        # Find best checkpoint
        query_embedding = np.random.randn(embedding_dim).astype(np.float32)
        results = db.search(query_embedding.tolist(), limit=3)

        print(f"‚úÖ W&B simulation successful: {len(results)} checkpoints retrieved")
        return True


def main():
    """Run all MLOps integration tests."""
    print("üöÄ OmenDB MLOps Integration Demo")
    print("=" * 50)

    tests = [
        ("NumPy Integration", test_numpy_integration),
        ("scikit-learn Integration", test_sklearn_compatibility),
        ("pandas Integration", test_pandas_integration),
        ("HuggingFace Simulation", test_huggingface_simulation),
        ("MLflow Integration", test_mlflow_integration),
        ("W&B Simulation", test_wandb_simulation),
    ]

    results = []

    for test_name, test_func in tests:
        print(f"\nüîß {test_name}")
        print("-" * 40)

        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"‚ùå {test_name} failed with error: {e}")
            results.append((test_name, False))

    # Summary
    print("\n" + "=" * 50)
    print("üìä MLOps Integration Summary")
    print("=" * 50)

    for test_name, success in results:
        status = "‚úÖ PASSED" if success else "‚ùå FAILED"
        print(f"{test_name}: {status}")

    passed = sum(1 for _, success in results if success)
    total = len(results)

    print(f"\nTotal: {passed}/{total} tests passed")

    if passed == total:
        print("\nüéâ All MLOps integrations working correctly!")
        print("\nOmenDB can be seamlessly integrated with:")
        print("‚Ä¢ NumPy for numerical computing")
        print("‚Ä¢ scikit-learn for ML pipelines")
        print("‚Ä¢ pandas for data processing")
        print("‚Ä¢ HuggingFace for embeddings")
        print("‚Ä¢ MLflow for experiment tracking")
        print("‚Ä¢ Weights & Biases for monitoring")
    else:
        print("\n‚ö†Ô∏è Some integrations need attention")


if __name__ == "__main__":
    main()
