#!/usr/bin/env python3
"""
Test @vectorize optimization impact properly by avoiding migration overhead.

Strategy:
1. Test pure brute-force (< 5K vectors, no migration)
2. Test large datasets (> 10K vectors, migration complete)
3. Compare before/after @vectorize implementation
"""

import time
import numpy as np
import sys
import os

# Add python directory to path
sys.path.insert(0, "python")

try:
    import omendb
except ImportError:
    print("‚ùå Failed to import omendb. Run: pixi run mojo build")
    sys.exit(1)


def test_brute_force_performance(num_vectors=4000, dimension=128, num_queries=500):
    """Test pure brute-force performance (no migration overhead)."""
    print(f"üî• Pure Brute-Force Test (No Migration)")
    print(
        f"   Vectors: {num_vectors:,}, Dimension: {dimension}, Queries: {num_queries:,}"
    )

    # Generate test data
    vectors = np.random.rand(num_vectors, dimension).astype(np.float32)
    ids = [f"vec_{i}" for i in range(num_vectors)]
    queries = np.random.rand(num_queries, dimension).astype(np.float32)

    # Test ingestion performance (pure brute-force)
    print("‚ö° Testing brute-force ingestion...")
    db = omendb.DB()

    batch_size = 1000
    total_time = 0

    for i in range(0, num_vectors, batch_size):
        end_idx = min(i + batch_size, num_vectors)
        batch_vectors = vectors[i:end_idx].tolist()
        batch_ids = ids[i:end_idx]
        batch_metadata = [{} for _ in range(end_idx - i)]

        start_time = time.time()
        db.add_batch(vectors=batch_vectors, ids=batch_ids, metadata=batch_metadata)
        batch_time = time.time() - start_time
        total_time += batch_time

    ingestion_rate = num_vectors / total_time
    print(f"   Ingestion: {ingestion_rate:,.0f} vectors/sec (pure brute-force)")

    # Test query performance
    print("üîç Testing query performance...")
    query_times = []

    for i in range(min(100, num_queries)):
        query = queries[i].tolist()

        start_time = time.time()
        results = db.search(query, limit=10)
        query_time = (time.time() - start_time) * 1000
        query_times.append(query_time)

    avg_query_time = np.mean(query_times[1:])  # Skip warmup
    qps = 1000 / avg_query_time

    print(f"   Average query: {avg_query_time:.3f}ms ({qps:.0f} QPS)")

    return {
        "type": "brute_force",
        "ingestion_rate": ingestion_rate,
        "avg_query_time": avg_query_time,
        "qps": qps,
    }


def test_large_dataset_performance(num_vectors=25000, dimension=128, num_queries=500):
    """Test large dataset performance (migration complete, stable state)."""
    print(f"\nüöÄ Large Dataset Test (Post-Migration Performance)")
    print(
        f"   Vectors: {num_vectors:,}, Dimension: {dimension}, Queries: {num_queries:,}"
    )

    # Generate test data
    vectors = np.random.rand(num_vectors, dimension).astype(np.float32)
    ids = [f"vec_{i}" for i in range(num_vectors)]
    queries = np.random.rand(num_queries, dimension).astype(np.float32)

    # Test ingestion performance
    print("‚ö° Testing large dataset ingestion...")
    db = omendb.DB()

    batch_size = 5000  # Larger batches for efficiency
    total_time = 0

    for i in range(0, num_vectors, batch_size):
        end_idx = min(i + batch_size, num_vectors)
        batch_vectors = vectors[i:end_idx].tolist()
        batch_ids = ids[i:end_idx]
        batch_metadata = [{} for _ in range(end_idx - i)]

        start_time = time.time()
        db.add_batch(vectors=batch_vectors, ids=batch_ids, metadata=batch_metadata)
        batch_time = time.time() - start_time
        total_time += batch_time

        if i == 0:
            print(
                f"   First batch: {(end_idx / batch_time):,.0f} vec/s (includes migration)"
            )

    ingestion_rate = num_vectors / total_time
    print(f"   Overall: {ingestion_rate:,.0f} vectors/sec (includes migration)")

    # Test query performance (should be stable HNSW performance)
    print("üîç Testing HNSW query performance...")
    query_times = []

    for i in range(min(200, num_queries)):
        query = queries[i].tolist()

        start_time = time.time()
        results = db.search(query, limit=10)
        query_time = (time.time() - start_time) * 1000
        query_times.append(query_time)

    avg_query_time = np.mean(query_times[5:])  # Skip warmup
    qps = 1000 / avg_query_time

    print(f"   Average query: {avg_query_time:.3f}ms ({qps:.0f} QPS)")

    return {
        "type": "large_dataset",
        "ingestion_rate": ingestion_rate,
        "avg_query_time": avg_query_time,
        "qps": qps,
    }


def test_conversion_intensive_workload():
    """Test workload that heavily uses the @vectorize optimized conversion."""
    print(f"\nüß™ Conversion-Intensive Test (@vectorize target)")

    # Use mixed dimension vectors to stress conversion
    dimensions = [64, 128, 256, 384, 512, 768, 1024]
    total_vectors = 0
    total_time = 0

    db = omendb.DB()

    for dim in dimensions:
        print(f"   Testing dimension {dim}...")
        vectors = np.random.rand(1000, dim).astype(
            np.float64
        )  # Float64 to force conversion

        start_time = time.time()
        for i in range(len(vectors)):
            db.add(f"vec_{dim}_{i}", vectors[i].tolist())
        conversion_time = time.time() - start_time

        rate = len(vectors) / conversion_time
        print(f"     {rate:,.0f} conversions/sec @ {dim}D")

        total_vectors += len(vectors)
        total_time += conversion_time

    overall_rate = total_vectors / total_time
    print(f"   Overall conversion rate: {overall_rate:,.0f} vectors/sec")

    return {
        "type": "conversion_intensive",
        "conversion_rate": overall_rate,
        "dimensions_tested": dimensions,
    }


def main():
    print("üöÄ @vectorize Optimization Performance Testing")
    print("=" * 60)
    print("Testing strategy: Avoid migration overhead, focus on @vectorize impact")
    print()

    # Test 1: Pure brute-force performance
    brute_force_results = test_brute_force_performance()

    # Test 2: Large dataset performance
    large_dataset_results = test_large_dataset_performance()

    # Test 3: Conversion-intensive workload
    conversion_results = test_conversion_intensive_workload()

    # Summary
    print(f"\nüìä @vectorize Optimization Results")
    print("=" * 60)
    print(
        f"üî• Pure Brute-Force:     {brute_force_results['ingestion_rate']:,.0f} vec/s"
    )
    print(
        f"üöÄ Large Dataset:        {large_dataset_results['ingestion_rate']:,.0f} vec/s"
    )
    print(
        f"üß™ Conversion-Intensive: {conversion_results['conversion_rate']:,.0f} vec/s"
    )
    print()
    print(f"üîç Query Performance:")
    print(
        f"   Brute-Force: {brute_force_results['avg_query_time']:.3f}ms ({brute_force_results['qps']:.0f} QPS)"
    )
    print(
        f"   HNSW:        {large_dataset_results['avg_query_time']:.3f}ms ({large_dataset_results['qps']:.0f} QPS)"
    )
    print()
    print("üéØ Key Insights:")
    print("   - @vectorize optimizes array conversion (Float64‚ÜíFloat32)")
    print("   - Hardware-aware worker counts active (15 workers)")
    print("   - Compiler automatically optimizes SIMD operations")


if __name__ == "__main__":
    main()
