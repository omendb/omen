#!/usr/bin/env python3
"""
Test metrics integration between Mojo engine and Python API.

Validates that metrics are properly collected and exported.
"""

import time
import json
from pathlib import Path
import tempfile
import random

# Import OmenDB
try:
    from omendb import DB

    OMENDB_AVAILABLE = True
except ImportError as e:
    print(f"‚ùå OmenDB not available: {e}")
    OMENDB_AVAILABLE = False


def test_metrics_basic_functionality():
    """Test basic metrics functionality."""
    print("\nüß™ Testing Basic Metrics Functionality")
    print("=" * 45)

    if not OMENDB_AVAILABLE:
        print("‚ö†Ô∏è Skipping - OmenDB not available")
        return False

    success = True

    with tempfile.TemporaryDirectory() as temp_dir:
        db_path = str(Path(temp_dir) / "metrics_test.omen")
        db = DB(db_path)

        try:
            # Test metrics export without any operations
            print("üìä Testing metrics export before operations...")

            prometheus_metrics = db.export_metrics("prometheus")
            json_metrics = db.export_metrics("json")
            statsd_metrics = db.export_metrics("statsd")
            health = db.get_health_status()

            # Validate formats
            assert isinstance(prometheus_metrics, str), (
                "Prometheus metrics should be string"
            )
            assert isinstance(json_metrics, str), "JSON metrics should be string"
            assert isinstance(statsd_metrics, str), "StatsD metrics should be string"
            assert isinstance(health, dict), "Health status should be dict"

            print("‚úÖ Metrics export working - formats validated")

            # Test that JSON is parseable
            metrics_data = json.loads(json_metrics)
            assert "metrics" in metrics_data, "JSON should contain metrics section"

            print("‚úÖ JSON metrics format is valid")

            # Test health status structure
            assert "status" in health, "Health should have status field"
            assert health["status"] in ["healthy", "unhealthy"], (
                "Status should be valid"
            )

            print("‚úÖ Health status format is valid")

        except Exception as e:
            print(f"‚ùå Basic functionality test failed: {e}")
            success = False
        finally:
            db.close()

    return success


def test_metrics_with_operations():
    """Test metrics collection during database operations."""
    print("\nüß™ Testing Metrics with Database Operations")
    print("=" * 48)

    if not OMENDB_AVAILABLE:
        print("‚ö†Ô∏è Skipping - OmenDB not available")
        return False

    success = True

    with tempfile.TemporaryDirectory() as temp_dir:
        db_path = str(Path(temp_dir) / "ops_metrics_test.omen")
        db = DB(db_path)

        try:
            # Get baseline metrics
            initial_metrics = db.export_metrics("json")
            initial_data = json.loads(initial_metrics)

            print("üìù Performing database operations...")

            # Perform insert operations
            test_vectors = [
                ("vec1", [1.0, 2.0, 3.0]),
                ("vec2", [4.0, 5.0, 6.0]),
                ("vec3", [7.0, 8.0, 9.0]),
            ]

            for vec_id, vector in test_vectors:
                success_flag = db.add(vec_id, vector)
                assert success_flag, f"Failed to add vector {vec_id}"

            print(f"‚úÖ Added {len(test_vectors)} vectors")

            # Perform query operations
            query_count = 5
            for i in range(query_count):
                query_vector = [random.uniform(1, 9) for _ in range(3)]
                results = db.search(query_vector, limit=2)
                assert len(results) > 0, f"Query {i} returned no results"

            print(f"‚úÖ Performed {query_count} queries")

            # Get updated metrics
            time.sleep(0.1)  # Brief pause to ensure metrics are updated
            final_metrics = db.export_metrics("json")
            final_data = json.loads(final_metrics)

            print("üìä Comparing metrics before and after operations...")

            # Extract metrics data
            initial_counters = initial_data.get("metrics", {}).get("counters", {})
            final_counters = final_data.get("metrics", {}).get("counters", {})

            print(f"Initial insert count: {initial_counters.get('inserts_total', 0)}")
            print(f"Final insert count: {final_counters.get('inserts_total', 0)}")
            print(f"Initial query count: {initial_counters.get('queries_total', 0)}")
            print(f"Final query count: {final_counters.get('queries_total', 0)}")

            # NOTE: This might still be 0 if native integration isn't complete
            # The test validates that the API works, even if metrics aren't connected yet
            print(
                "‚úÖ Metrics API functional (values may be 0 until native integration complete)"
            )

        except Exception as e:
            print(f"‚ùå Operations test failed: {e}")
            success = False
        finally:
            db.close()

    return success


def test_metrics_export_formats():
    """Test different metrics export formats."""
    print("\nüß™ Testing Metrics Export Formats")
    print("=" * 35)

    if not OMENDB_AVAILABLE:
        print("‚ö†Ô∏è Skipping - OmenDB not available")
        return False

    success = True

    with tempfile.TemporaryDirectory() as temp_dir:
        db_path = str(Path(temp_dir) / "format_test.omen")
        db = DB(db_path)

        try:
            # Test Prometheus format
            print("üìä Testing Prometheus format...")
            prometheus = db.export_metrics("prometheus")

            # Validate Prometheus format
            assert "# HELP" in prometheus, "Prometheus should contain HELP comments"
            assert "# TYPE" in prometheus, "Prometheus should contain TYPE comments"
            assert "omendb_" in prometheus, "Prometheus should contain omendb metrics"

            print("‚úÖ Prometheus format valid")

            # Test JSON format
            print("üìä Testing JSON format...")
            json_str = db.export_metrics("json")
            json_data = json.loads(json_str)  # Should parse without error

            assert "database_id" in json_data, "JSON should contain database_id"
            assert "timestamp" in json_data, "JSON should contain timestamp"
            assert "metrics" in json_data, "JSON should contain metrics section"

            print("‚úÖ JSON format valid")

            # Test StatsD format
            print("üìä Testing StatsD format...")
            statsd = db.export_metrics("statsd")

            # Validate StatsD format (should have |c or |g indicators)
            assert "|" in statsd, "StatsD should contain type indicators"
            assert "omendb." in statsd, "StatsD should contain omendb namespace"

            print("‚úÖ StatsD format valid")

            # Test invalid format handling
            print("üìä Testing error handling...")
            try:
                db.export_metrics("invalid_format")
                print("‚ùå Should have raised error for invalid format")
                success = False
            except Exception:
                print("‚úÖ Invalid format properly rejected")

        except Exception as e:
            print(f"‚ùå Format test failed: {e}")
            success = False
        finally:
            db.close()

    return success


def test_health_status():
    """Test health status functionality."""
    print("\nüß™ Testing Health Status")
    print("=" * 25)

    if not OMENDB_AVAILABLE:
        print("‚ö†Ô∏è Skipping - OmenDB not available")
        return False

    success = True

    with tempfile.TemporaryDirectory() as temp_dir:
        db_path = str(Path(temp_dir) / "health_test.omen")
        db = DB(db_path)

        try:
            health = db.get_health_status()

            # Validate health status structure
            required_fields = ["status", "uptime_seconds", "memory_mb", "success_rate"]
            for field in required_fields:
                assert field in health, f"Health status should contain {field}"

            # Validate status values
            assert health["status"] in ["healthy", "unhealthy"], (
                "Status should be valid"
            )
            assert isinstance(health["uptime_seconds"], (int, float)), (
                "Uptime should be numeric"
            )
            assert isinstance(health["memory_mb"], (int, float)), (
                "Memory should be numeric"
            )
            assert isinstance(health["success_rate"], (int, float)), (
                "Success rate should be numeric"
            )

            print("‚úÖ Health status structure is valid")

            # Test that uptime increases over time
            initial_uptime = health["uptime_seconds"]
            time.sleep(0.1)
            new_health = db.get_health_status()
            new_uptime = new_health["uptime_seconds"]

            assert new_uptime > initial_uptime, "Uptime should increase over time"
            print("‚úÖ Uptime tracking working")

        except Exception as e:
            print(f"‚ùå Health status test failed: {e}")
            success = False
        finally:
            db.close()

    return success


def test_performance_impact():
    """Test that metrics collection has minimal performance impact."""
    print("\nüß™ Testing Performance Impact")
    print("=" * 32)

    if not OMENDB_AVAILABLE:
        print("‚ö†Ô∏è Skipping - OmenDB not available")
        return False

    success = True

    with tempfile.TemporaryDirectory() as temp_dir:
        db_path = str(Path(temp_dir) / "perf_test.omen")
        db = DB(db_path)

        try:
            # Time metrics export operations
            export_times = []

            for _ in range(100):
                start_time = time.time()
                db.export_metrics("prometheus")
                export_time = (time.time() - start_time) * 1000  # ms
                export_times.append(export_time)

            avg_export_time = sum(export_times) / len(export_times)
            max_export_time = max(export_times)

            print(f"üìä Metrics export performance:")
            print(f"  Average: {avg_export_time:.2f}ms")
            print(f"  Maximum: {max_export_time:.2f}ms")

            # Metrics export should be fast (< 10ms average)
            if avg_export_time < 10.0:
                print("‚úÖ Metrics export performance acceptable")
            else:
                print(
                    f"‚ö†Ô∏è Metrics export might be slow: {avg_export_time:.2f}ms average"
                )
                # Don't fail the test, but warn about performance

            # Time health status operations
            health_times = []

            for _ in range(100):
                start_time = time.time()
                db.get_health_status()
                health_time = (time.time() - start_time) * 1000  # ms
                health_times.append(health_time)

            avg_health_time = sum(health_times) / len(health_times)
            print(f"üìä Health status performance: {avg_health_time:.2f}ms average")

            if avg_health_time < 5.0:
                print("‚úÖ Health status performance acceptable")
            else:
                print(f"‚ö†Ô∏è Health status might be slow: {avg_health_time:.2f}ms average")

        except Exception as e:
            print(f"‚ùå Performance test failed: {e}")
            success = False
        finally:
            db.close()

    return success


def run_all_tests():
    """Run all metrics integration tests."""
    print("üöÄ OmenDB Metrics Integration Tests")
    print("=" * 40)

    if not OMENDB_AVAILABLE:
        print("‚ùå Cannot run tests - OmenDB not available")
        return False

    tests = [
        ("Basic Functionality", test_metrics_basic_functionality),
        ("Operations Metrics", test_metrics_with_operations),
        ("Export Formats", test_metrics_export_formats),
        ("Health Status", test_health_status),
        ("Performance Impact", test_performance_impact),
    ]

    results = []

    for test_name, test_func in tests:
        print(f"\n{'=' * 60}")
        print(f"Running: {test_name}")
        print("=" * 60)

        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"‚ùå Test {test_name} failed with exception: {e}")
            results.append((test_name, False))

    # Summary
    print(f"\n{'=' * 60}")
    print("TEST SUMMARY")
    print("=" * 60)

    passed = 0
    total = len(results)

    for test_name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status} {test_name}")
        if result:
            passed += 1

    print(f"\nResults: {passed}/{total} tests passed")

    if passed == total:
        print("‚úÖ All metrics integration tests passed!")
        return True
    else:
        print(f"‚ùå {total - passed} tests failed")
        return False


if __name__ == "__main__":
    success = run_all_tests()
    exit(0 if success else 1)
