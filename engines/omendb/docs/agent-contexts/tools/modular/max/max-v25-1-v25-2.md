# Modular MAX Technical Documentation

## Metadata Section
```
TITLE: Modular MAX
VERSION: v25.1 - v25.2 nightly
RELEASED: 2025-02-13 (v25.1) to current (v25.2 nightly)
COMPATIBILITY: NVIDIA GPUs (A100, A10, L4, L40, etc.), CPU
DOCUMENTATION_SOURCE: max-changelog.md
MODEL: Claude-3.7-Sonnet-Thinking
```

## Conceptual Overview
- Multi-GPU support expanded with tensor parallelism for large language models like Llama-3.3-70B
- Optimized batch scheduling with prefix cache awareness and in-flight batching capabilities
- New GPU programming examples for complex operations like fused attention and matrix multiplication
- Support for additional model architectures including Phi-3.5/4, EXAONE, Olmo, and Granite
- Memory efficiency improvements with copy-on-write KV blocks and GPTQ quantization support

## Core Features

### Multi-GPU Support with Tensor Parallelism [`STABLE`]

**Package:** `max.pipelines`
**Available Since:** `v25.2 nightly`
**Status:** Stable

**Signature:**
```bash
max-pipelines generate --model-path=<model-path> \
  --quantization-encoding bfloat16 \
  --devices gpu:0,1,2,3 \
  --prompt="<your prompt here>"
```

**Usage Example:**
```bash
max-pipelines generate --model-path=meta-llama/Llama-3.3-70B-Instruct \
  --quantization-encoding bfloat16 \
  --devices gpu:0,1,2,3 \
  --prompt="Design a self-sustaining colony on Neptune's moon Triton with a myth/science fusion name."
```

**Context:**
- Purpose: Enables running large language models that wouldn't fit in a single GPU's memory
- Patterns: Uses tensor parallelism to automatically split model execution across multiple GPUs
- Limitations: Current implementation is for LlamaForCausalLM architecture models
- Related: Previous single-GPU execution model

**Migration:**
```bash
# BEFORE:
max-pipelines generate --model-path=model-name --devices=gpu-0

# AFTER:
max-pipelines generate --model-path=model-name --devices=gpu:0,1,2,3
```
**Migration Difficulty:** Simple

### Extended MAX Serve Batch Scheduling [`STABLE`]

**Package:** `max.serve`
**Available Since:** `v25.2 nightly`
**Status:** Stable

**Context:**
- Purpose: Improves throughput by optimizing batch creation based on prefix cache state
- Patterns: Creates larger batches when many prompt tokens are already cached
- Performance: Improves throughput up to 10% in some benchmarks
- Related: Prefix caching, PagedAttention

### In-Flight Batching [`STABLE`]

**Package:** `max.serve`
**Available Since:** `v25.2 nightly`
**Status:** Stable

**Signature:**
```bash
max-serve --enable-in-flight-batch
```

**Context:**
- Purpose: Reduces inter-token latency by scheduling token generation requests alongside context encoding
- Patterns: Combines prompt processing with token generation in a single batch
- Related: Batch scheduling, continuous batching

### Copy-on-Write KV Blocks [`STABLE`]

**Package:** `max.serve`
**Available Since:** `v25.2 nightly`
**Status:** Stable

**Context:**
- Purpose: Improves prefix cache hit rate and prefill performance
- Patterns: Used with PagedAttention and Prefix Caching
- Performance: Reduces memory usage when multiple requests share common prefixes

### GPTQ Quantization Support [`STABLE`]

**Package:** `max.pipelines`
**Available Since:** `v25.2 nightly`
**Status:** Stable

**Signature:**
```bash
max-pipelines generate --model-path <model-repo> --quantization-encoding gptq
```

**Usage Example:**
```bash
max-pipelines generate \
--model-path hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4 \
--prompt "Why is the sky blue?" \
--max-batch-size 1 --max-length 10000 \
--quantization-encoding gptq
```

**Context:**
- Purpose: Reduces memory usage while maintaining reasonable performance
- Performance: Reduces total memory consumption from ~16 GB to ~5 GB for Llama 3.1 8B
- Limitations: Currently only for models running on GPU
- Related: Other quantization techniques like bfloat16

### Structured Output Improvements [`STABLE`]

**Package:** `max.serve`
**Available Since:** `v25.2 nightly`
**Status:** Stable

**Context:**
- Purpose: Reduces token counts and improves model adherence to output formats
- Features:
  - Constraints on whitespace during Structured Output
  - Jump ahead decoding that auto-completes tokens when a singular path forward is identified
- Performance: Improves completion times by up to ~20% for long prompts

### MAX Autoregressive Model Performance Fix [`STABLE`]

**Package:** `max.serve`
**Available Since:** `v25.1.1`
**Status:** Stable

**Context:**
- Purpose: Fixes performance issues in autoregressive models with paged attention
- Changes: Sets sensible platform-specific default values for --max-num-steps
- Related: PagedAttention, autoregressive inference

## APIs

### max.ops.chunk [`STABLE`]

**Package:** `max.ops`
**Available Since:** `v25.2 nightly`
**Status:** Stable

**Context:**
- Purpose: Supports chunking tensors along an axis
- Related: Similar tensor operations like split, slice

### Accelerator.can_access [`STABLE`]

**Package:** `max.driver`
**Available Since:** `v25.2 nightly`
**Status:** Stable

**Signature:**
```python
def can_access(other: Device) -> bool
```

**Context:**
- Purpose: Checks if one device can directly access memory of another device
- Use Cases: Multi-device setups, determining memory access capabilities between devices

### Device.synchronize [`STABLE`]

**Package:** `max.driver`
**Available Since:** `v25.2 nightly`
**Status:** Stable

**Signature:**
```python
def synchronize()
```

**Context:**
- Purpose: Ensures all operations on the device complete before returning
- Patterns: Used for synchronization between host and device operations
- Related: Asynchronous execution model

### max.driver.Tensor.item [`STABLE`]

**Package:** `max.driver`
**Available Since:** `v25.2 nightly`
**Status:** Stable

**Context:**
- Purpose: Extracts single value from a tensor
- Pattern Changes: Now works on any single-element tensor (previously restricted to rank-0 tensors)

### DeviceContextPtr [`STABLE`]

**Package:** `max.driver`
**Available Since:** `v25.2 nightly`
**Status:** Stable
**Breaking:** Yes

**Context:**
- Purpose: Directly exposes the device context pointer for custom ops
- Migration: Replaces MojoCallContextPtr which only contained a DeviceContextPtr

**Migration:**
```python
# BEFORE:
@staticmethod
fn execute[type: DType, rank: Int](
    output: ManagedTensorSlice[type=type, rank=rank],
    input: ManagedTensorSlice[type=type, rank=rank],
    ctx: MojoCallContextPtr,
):
    # implementation

# AFTER:
@staticmethod
fn execute[type: DType, rank: Int](
    output: ManagedTensorSlice[type=type, rank=rank],
    input: ManagedTensorSlice[type=type, rank=rank],
    ctx: DeviceContextPtr,
):
    # implementation
```
**Migration Difficulty:** Simple

### Direct Function Enqueuing [`STABLE`]

**Package:** `max.driver`
**Available Since:** `v25.2 nightly`
**Status:** Stable

**Signature:**
```python
ctx.enqueue_function[func](grid_dim=1, block_dim=1)
```

**Usage Example:**
```python
fn func():
    print("Hello from GPU")

@register("custom_op")
struct CustomOp:
    @staticmethod
    fn execute(ctx: DeviceContextPtr) raises:
        var dev_ctx = ctx.get_device_context()
        dev_ctx.enqueue_function[func](grid_dim=1, block_dim=1)
```

**Context:**
- Purpose: Simplifies GPU kernel enqueuing by allowing direct function passing
- Patterns: Useful for quick one-off kernels
- Performance: Incurs 50-500 nanoseconds overhead per enqueue compared to compiled functions
- Alternatives: For repeated usage, compiling the function first is more efficient

**Edge Cases and Anti-patterns:**
```python
# ANTI-PATTERN (for repeated calls):
for i in range(100):
    dev_ctx.enqueue_function[func](grid_dim=1, block_dim=1)

# CORRECT (for repeated calls):
var compiled_func = ctx.compile_function[func]()
for i in range(100):
    ctx.enqueue_function(compiled_func, grid_dim=1, block_dim=1)
```

### Accelerator and CPU Type Changes [`STABLE`]

**Package:** `max.driver`
**Available Since:** `v25.2 nightly`
**Status:** Stable
**Breaking:** Yes

**Context:**
- Purpose: Makes Accelerator and CPU actual types in Python instead of factory methods
- Benefits: Enables proper type annotations in Python

**Migration:**
```python
# BEFORE:
graph_devices: list[DeviceRef]

# AFTER:
graph_devices: list[Accelerator]
```
**Migration Difficulty:** Simple

### ops.min and ops.max with Axis Reduction [`STABLE`]

**Package:** `max.ops`
**Available Since:** `v25.2 nightly`
**Status:** Stable

**Signature:**
```python
ops.min(tensor, axis=-1)
ops.max(tensor, axis=-1)
```

**Context:**
- Purpose: Enables reduction operations along specified axes
- Related: Other reduction operations like sum, mean

### gelu with Approximation Options [`STABLE`]

**Package:** `max.ops`
**Available Since:** `v25.2 nightly`
**Status:** Stable

**Signature:**
```python
ops.gelu(x, approximate='none'|'tanh'|'fast')
```

**Context:**
- Purpose: Provides different approximation methods for the GELU activation function
- Options:
  - 'none': Standard GELU implementation
  - 'tanh': Approximation using tanh: 0.5 * x * (1.0 + tanh(0.7978845608028654 * (x + 0.044715 * x**3)))
  - 'fast': Faster approximation using sigmoid: sigmoid(1.702 * x) * x
- Use Cases: Performance vs accuracy tradeoffs in neural networks

## Breaking Changes

### Removal of PipelineConfig.architecture [`REMOVED`]

**Package:** `max.pipelines`
**Available Since:** `v25.2 nightly`
**Status:** Removed
**Breaking:** Yes

**Context:**
- Purpose: Simplifies configuration by removing redundant architecture specification
- Migration: Models are now automatically detected based on their structure

### Removal of roundeven Operation [`REMOVED`]

**Package:** `max.ops`
**Available Since:** `v25.2 nightly`
**Status:** Removed
**Breaking:** Yes

**Context:**
- Purpose: Simplifies API by using round operation for all cases
- Migration: Use round which now has the same behavior as roundeven

### Removal of Elementwise Operations from Tensor [`REMOVED`]

**Package:** `max.tensor_internal`
**Available Since:** `v25.2 nightly`
**Status:** Removed
**Breaking:** Yes

**Context:**
- Purpose: Part of phasing out the tensor_internal.Tensor in favor of LayoutTensor
- Migration: Use LayoutTensor and its operations instead

### Device Format Changes for --devices Argument [`CHANGED`]

**Package:** `max.pipelines`
**Available Since:** `v25.2 nightly`
**Status:** Changed
**Breaking:** Yes

**Migration:**
```bash
# BEFORE:
max-pipelines generate --model-path=model-name --devices=gpu-0,gpu-1

# AFTER:
max-pipelines generate --model-path=model-name --devices=gpu:0,1
```
**Migration Difficulty:** Simple

### Module Import Changes [`CHANGED`]

**Package:** `max.entrypoints`
**Available Since:** `v25.2 nightly`
**Status:** Changed
**Breaking:** Yes

**Migration:**
```python
# BEFORE:
from max.entrypoints import LLM

# AFTER:
from max.entrypoints.llm import LLM
```
**Migration Difficulty:** Simple

### Consolidation of --model-path and --weight-path [`CHANGED`]

**Package:** `max.pipelines`
**Available Since:** `v25.2 nightly`
**Status:** Changed
**Breaking:** Yes

**Context:**
- Purpose: Simplifies configuration by consolidating model and weight path parameters
- Behavior: If valid --weight-path(s) are provided, they override --model-path
- Migration: Set --model-path explicitly when needed

## New Models and Architectures

### Phi Models Support [`STABLE`]

**Package:** `max.pipelines`
**Available Since:** `v25.2 nightly`
**Status:** Stable

**Usage Example:**
```bash
max-pipelines generate \
  --model-path=microsoft/phi-4 \
  --prompt="Write bubble sort in mojo"
```

**Context:**
- Purpose: Adds support for the Phi3ForCausalLM model architecture
- Models: microsoft/Phi-3.5-mini-instruct, microsoft/phi-4
- Related: Other causal language models in MAX

### OlmoForCausalLM Support [`STABLE`]

**Package:** `max.pipelines`
**Available Since:** `v25.2 nightly`
**Status:** Stable

**Usage Example:**
```bash
max-pipelines generate \
  --model-path=allenai/OLMo-1B-0724-hf \
  --prompt="Write bubble sort in mojo"
```

**Context:**
- Purpose: Adds support for the OlmoForCausalLM model architecture
- Related: Other causal language models in MAX

### GraniteForCausalLM Support [`STABLE`]

**Package:** `max.pipelines`
**Available Since:** `v25.2 nightly`
**Status:** Stable

**Usage Example:**
```bash
max-pipelines generate \
  --model-path=ibm-granite/granite-3.1-8b-instruct \
  --prompt="Write bubble sort in mojo"
```

**Context:**
- Purpose: Adds support for the GraniteForCausalLM model architecture
- Related: Other causal language models in MAX

### EXAONE Models Support [`STABLE`]

**Package:** `max.pipelines`
**Available Since:** `v25.2 nightly`
**Status:** Stable

**Usage Example:**
```bash
max-pipelines generate \
  --model-path=LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct \
  --prompt="Write bubble sort in mojo"
```

**Context:**
- Purpose: Adds support for LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct and LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct
- Related: Other causal language models in MAX

## Advanced Usage

### Custom Operation GPU Programming Examples [`EXPERIMENTAL`]

**Package:** `max.examples`
**Available Since:** `v25.2 nightly`
**Status:** Experimental

**Context:**
- Purpose: Demonstrates advanced GPU programming techniques using MAX abstractions
- Examples:
  - fused_attention: Shows complex GPU programming for attention mechanisms
  - matrix_multiplication: Series of progressive optimizations for matrix multiplications
  - histogram: Implementation of histogram pattern as a custom op

### --huggingface-revision Option [`STABLE`]

**Package:** `max.pipelines`
**Available Since:** `v25.2 nightly`
**Status:** Stable

**Signature:**
```bash
max-pipelines generate --model-path=model-name --huggingface-revision=branch-or-commit
```

**Context:**
- Purpose: Allows selecting non-default branches or specific commits in Hugging Face repositories
- Use Cases: Testing specific versions, accessing development branches

### Parallel Model Weight Download [`STABLE`]

**Package:** `max.pipelines`
**Available Since:** `v25.2 nightly`
**Status:** Stable

**Context:**
- Purpose: Improves download performance for model weights
- Performance: Significantly reduces download times for large models

## Integration with Other Libraries

### Transformers v4.49.0 Support [`STABLE`]

**Package:** `max.serve`
**Available Since:** `v25.2 nightly`
**Status:** Stable

**Context:**
- Purpose: Ensures compatibility with latest Transformers library
- Features: Includes patch to avoid graph breaks when using torch.compile on Llama models
