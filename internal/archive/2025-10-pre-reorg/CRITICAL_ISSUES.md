# CRITICAL ISSUES - Deep Analysis
*2025-02-11 - Why HNSW+ is underperforming*

## ðŸ”´ THE REAL PROBLEM

We moved from a working system to HNSW+ and lost massive performance. The issue is NOT the algorithm - it's our implementation has fundamental problems that we can fix NOW.

## COMPARISON: Before vs After

### Before (DiskANN/Earlier Implementation)
- **Insert**: Was achieving 3,000-5,000 vec/s
- **Scale**: Could handle 25K-50K vectors
- **Memory**: More efficient per vector
- **Stability**: Working in production scenarios

### After (Current HNSW+)
- **Insert**: 907 vec/s (3-5x SLOWER)
- **Scale**: Limited to 100K fixed capacity
- **Memory**: 11.8KB/vector (3x overhead)
- **Stability**: Multiple crashes and memory issues

## ROOT CAUSE ANALYSIS

### 1. Implementation Issues We Can Fix NOW

#### âŒ PROBLEM: Fixed 100K Capacity
```mojo
# Current - hardcoded limit
alias VECTOR_CAPACITY = 100000  # Line 78 in hnsw.mojo
```
**FIX**: Implement dynamic growth
- Start with 10K capacity
- Grow by 2x when 80% full
- Reallocate and migrate (like std::vector)

#### âŒ PROBLEM: Inefficient Graph Construction
```mojo
# Current - exploring too many candidates
var ef = max(M * 2, 32)  # Still exploring 32+ nodes per insertion
```
**FIX**: More aggressive pruning
- Reduce ef during construction to M (not M*2)
- Early termination when good enough neighbor found
- Skip distant candidates entirely

#### âŒ PROBLEM: Memory Allocation Per Operation
```mojo
# Current - allocating memory in hot path
var candidates = KNNBuffer(ef)  # Allocates every search!
var W = KNNBuffer(ef)           # Another allocation!
```
**FIX**: Object pooling
- Pre-allocate search buffers
- Reuse across operations
- Thread-local pools for parallel access

#### âŒ PROBLEM: Binary Quantization Overhead
```mojo
# Current - creating binary vectors during insertion
var binary_vec = BinaryQuantizedVector(vector, self.dimension)
```
**FIX**: Lazy quantization
- Only create binary vectors when needed
- Cache and reuse
- Background thread for batch quantization

### 2. Mojo Limitations (Must Work Around)

#### NodePool Memory Corruption
**WORKAROUND**: Don't use complex nested structures
- Flatten to single allocation
- Use indices instead of pointers
- Fixed-size pools with simple management

#### No True Parallelism Yet
**WORKAROUND**: Optimize single-threaded path
- Better cache locality
- Minimize allocations
- Batch operations where possible

## IMMEDIATE ACTION PLAN

### Phase 1: Fix Critical Issues (1-2 days)
1. **Dynamic Capacity**
   ```mojo
   fn grow(mut self):
       var new_capacity = self.capacity * 2
       var new_vectors = UnsafePointer[Float32].alloc(new_capacity * self.dimension)
       memcpy(new_vectors, self.vectors, self.capacity * self.dimension * 4)
       self.vectors.free()
       self.vectors = new_vectors
       self.capacity = new_capacity
   ```

2. **Reduce Search Overhead**
   - Change ef from M*2 to M during construction
   - Add early termination threshold
   - Skip candidates > 2x best distance

3. **Object Pooling**
   ```mojo
   struct SearchBufferPool:
       var buffers: List[KNNBuffer]
       
       fn get(mut self) -> KNNBuffer:
           if len(self.buffers) > 0:
               return self.buffers.pop()
           return KNNBuffer(self.ef)
   ```

### Phase 2: Optimize Hot Path (2-3 days)
1. **Batch Processing**
   - Group insertions for better cache usage
   - Amortize search overhead
   - Vectorize distance calculations

2. **Memory Layout**
   - Align vectors to cache lines (64 bytes)
   - Pack graph structure better
   - Remove unnecessary indirection

3. **Smarter Graph Construction**
   - Probabilistic acceptance of neighbors
   - Limit search depth adaptively
   - Use approximate distances when far

### Phase 3: Architecture Improvements (1 week)
1. **Streaming Buffer Pattern**
   - Write-ahead log for persistence
   - Background graph construction
   - Decouple insertion from indexing

2. **Tiered Storage**
   - Hot tier: Recent vectors in memory
   - Warm tier: Indexed in graph
   - Cold tier: On disk, loaded on demand

## EXPECTED RESULTS

With these fixes:
- **Insert**: 10,000-20,000 vec/s (10-20x improvement)
- **Scale**: Unlimited (dynamic growth)
- **Memory**: 6-8KB/vector (2x improvement)
- **Stability**: Production ready

## WHY THIS WILL WORK

1. **Other HNSW implementations achieve 25K-500K vec/s** - The algorithm is proven
2. **Our earlier version was faster** - We know it's possible
3. **Issues are implementation, not fundamental** - All fixable
4. **Mojo is fast enough** - Just need better patterns

## COMPARISON TO COMPETITORS

| System | Insert Rate | Our Gap | Why They're Faster |
|--------|------------|---------|-------------------|
| Qdrant | 500K vec/s | 550x | Parallel insertion + better memory |
| Weaviate | 200K vec/s | 220x | Efficient graph construction |
| Pinecone | 25K vec/s | 27x | Optimized single-threaded path |
| **OmenDB (current)** | 907 vec/s | - | Poor implementation |
| **OmenDB (with fixes)** | 20K vec/s | 2.5x | Achievable target |

## THE VERDICT

**We don't need to wait for Mojo improvements.** The current performance gap is due to poor implementation choices that we can fix immediately. The earlier system was working better because it avoided these pitfalls.

**Priority**: Fix implementation issues first, optimize later.