"""
Segmented HNSW Architecture Design
October 2025

Goal: Achieve 15-25K vec/s insertion with 95% recall
Strategy: Independent segments built in parallel, merged at query time
"""

from math import ceil
from algorithm import parallelize

# Configuration based on research
alias SEGMENT_SIZE = 5000          # Optimal for cache locality
alias MAX_SEGMENTS = 32            # Limit for merge complexity
alias PARALLEL_WORKERS = 8         # 8-16 optimal per Qdrant
alias INDEXING_THRESHOLD = 1000    # Rebuild if >1K unindexed

struct HNSWSegment:
    """Single HNSW segment - independent graph"""
    var segment_id: Int
    var hnsw: HNSW  # Existing HNSW implementation
    var indexed_count: Int
    var unindexed_buffer: List[Float32]  # For incremental updates
    var is_building: Bool  # Lock-free flag for concurrent access

    fn __init__(inout self, segment_id: Int, dimension: Int):
        self.segment_id = segment_id
        self.hnsw = HNSW(dimension, SEGMENT_SIZE)
        self.indexed_count = 0
        self.unindexed_buffer = List[Float32]()
        self.is_building = False

    fn insert_batch(inout self, vectors: UnsafePointer[Float32], count: Int):
        """Build segment sequentially (maintains quality)"""
        # Sequential insertion within segment preserves HNSW properties
        for i in range(count):
            var vec_ptr = vectors.offset(i * self.hnsw.dimension)
            self.hnsw.insert_single(vec_ptr)
            self.indexed_count += 1

    fn search(self, query: UnsafePointer[Float32], k: Int) -> List[SearchResult]:
        """Search this segment"""
        return self.hnsw.search(query, k)

struct SearchResult:
    """Result from segment search"""
    var segment_id: Int
    var node_id: Int
    var distance: Float32
    var global_id: String  # Original ID

struct SegmentedHNSW:
    """
    Main segmented HNSW implementation
    Key insight: Parallelize the problem, not the algorithm
    """
    var dimension: Int
    var segments: List[HNSWSegment]
    var active_segment_idx: Int
    var total_vectors: Int
    var id_to_segment: Dict[String, Int]  # Map ID to segment

    fn __init__(inout self, dimension: Int):
        self.dimension = dimension
        self.segments = List[HNSWSegment]()
        self.active_segment_idx = -1
        self.total_vectors = 0
        self.id_to_segment = Dict[String, Int]()

    fn insert_batch(inout self,
                   ids: List[String],
                   vectors: UnsafePointer[Float32],
                   n_vectors: Int) -> List[Int]:
        """
        Parallel segment construction - the key to performance
        """
        var results = List[Int]()

        # Calculate segment distribution
        var vectors_per_segment = min(SEGMENT_SIZE, n_vectors)
        var num_segments = ceil(Float32(n_vectors) / Float32(vectors_per_segment))
        num_segments = min(num_segments, MAX_SEGMENTS)

        print("ðŸ“¦ SEGMENTED HNSW: Creating", num_segments, "segments")
        print("   Vectors per segment:", vectors_per_segment)
        print("   Parallel workers:", min(PARALLEL_WORKERS, num_segments))

        # Pre-allocate segments
        var start_idx = len(self.segments)
        for i in range(num_segments):
            self.segments.append(HNSWSegment(start_idx + i, self.dimension))

        # Build segments in parallel - TRUE PARALLELISM!
        @parameter
        fn build_segment(segment_idx: Int):
            """Build one segment independently"""
            var start = segment_idx * vectors_per_segment
            var end = min(start + vectors_per_segment, n_vectors)
            var count = end - start

            if count <= 0:
                return

            var segment_id = start_idx + segment_idx
            var segment = self.segments[segment_id]

            # Mark as building (lock-free flag)
            segment.is_building = True

            # Build HNSW for this segment (sequential within segment)
            var segment_vectors = vectors.offset(start * self.dimension)
            segment.insert_batch(segment_vectors, count)

            # Map IDs to segment
            for i in range(start, end):
                self.id_to_segment[ids[i]] = segment_id

            segment.is_building = False

            print("   âœ… Segment", segment_idx, "complete:", count, "vectors")

        # Execute parallel build
        parallelize[build_segment](num_segments, min(PARALLEL_WORKERS, num_segments))

        self.total_vectors += n_vectors
        print("ðŸŽ¯ SEGMENTED BUILD COMPLETE:", n_vectors, "vectors across", num_segments, "segments")

        # Return success for all vectors
        for i in range(n_vectors):
            results.append(i)

        return results

    fn search(self, query: UnsafePointer[Float32], k: Int) -> List[SearchResult]:
        """
        Parallel search across segments with merge
        """
        var all_results = List[List[SearchResult]]()

        # Search all segments in parallel
        @parameter
        fn search_segment(segment_idx: Int):
            if segment_idx >= len(self.segments):
                return

            var segment = self.segments[segment_idx]
            if segment.is_building:
                return  # Skip segments being built

            var segment_results = segment.search(query, k)
            all_results.append(segment_results)

        # Parallel search
        parallelize[search_segment](len(self.segments), PARALLEL_WORKERS)

        # Merge results - take top k across all segments
        return self._merge_results(all_results, k)

    fn _merge_results(self,
                     segment_results: List[List[SearchResult]],
                     k: Int) -> List[SearchResult]:
        """Merge and deduplicate results from segments"""
        var merged = List[SearchResult]()

        # Simple merge - in production use a heap
        for segment_list in segment_results:
            for result in segment_list:
                merged.append(result)

        # Sort by distance
        # TODO: Implement proper sorting
        # merged.sort(key=lambda r: r.distance)

        # Return top k
        var final_results = List[SearchResult]()
        var count = min(k, len(merged))
        for i in range(count):
            final_results.append(merged[i])

        return final_results

    fn optimize_segments(inout self):
        """
        Merge small segments for efficiency (background task)
        Based on Qdrant's approach
        """
        # Non-linear build time: 2 segments of N faster than 1 of 2N
        # So we merge carefully

        var small_segments = List[Int]()
        for i in range(len(self.segments)):
            if self.segments[i].indexed_count < SEGMENT_SIZE // 2:
                small_segments.append(i)

        if len(small_segments) >= 2:
            print("ðŸ”„ Merging", len(small_segments), "small segments")
            # TODO: Implement segment merging with graph reuse

# Performance projections based on research:
# - Build: 15-25K vec/s (8 workers Ã— 2-3K vec/s per worker)
# - Search: ~2ms latency (parallel segment search + merge)
# - Recall: 95%+ (each segment maintains HNSW quality)
# - Memory: Similar to monolithic (slight overhead for segment metadata)

"""
Implementation Plan:
1. Replace monolithic HNSW with SegmentedHNSW in native.mojo
2. Test with varying segment sizes (1K, 5K, 10K)
3. Optimize merge algorithm (use heap for efficiency)
4. Add segment merging for long-term efficiency
5. Profile and tune PARALLEL_WORKERS

Expected Results:
- 20x speedup over sequential (735 â†’ 15K+ vec/s)
- Maintained quality (94%+ recall)
- Linear scaling with cores
- Production-ready architecture
"""